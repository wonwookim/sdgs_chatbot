from openai import OpenAI
from dotenv import load_dotenv
import os
import chromadb
from chromadb.utils import embedding_functions
import json
from pathlib import Path
from typing import Optional, Dict, List
from sentence_transformers import CrossEncoder
import numpy as np
import torch
from torch.nn.functional import sigmoid

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

client = OpenAI(api_key=api_key)

# Cross-encoder ëª¨ë¸ ì´ˆê¸°í™”
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def expand_query(query: str, min_length: int = 10) -> str:
    """ì§§ì€ ì¿¼ë¦¬ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ í™•ì¥"""
    if len(query) > min_length: # ì¿¼ë¦¬ê°€ ìµœì†Œ ê¸¸ì´ë³´ë‹¤ í¬ë©´ ì¿¼ë¦¬ ë°˜í™˜
        return query
        
    system_prompt = """ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§§ì€ ì§ˆë¬¸ì„ ESG ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë” êµ¬ì²´ì ì´ê³  í’ë¶€í•˜ê²Œ ë°”ê¿”ì£¼ëŠ” ì „ë¬¸ê°€ì•¼.
    ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì•¼ í•´:
    1. ì›ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í™•ì¥
    2. ESG ê´€ë ¨ êµ¬ì²´ì ì¸ ìš©ì–´ë‚˜ ê°œë… í¬í•¨
    3. ê²€ìƒ‰ì— ë„ì›€ë  ë§Œí•œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
    4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
    5. í™•ì¥ëœ ì§ˆë¬¸ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œí•œ
    
    ì˜ˆì‹œ:
    ì…ë ¥: "íƒ„ì†Œë°°ì¶œëŸ‰?"
    ì¶œë ¥: "ê¸°ì—…ì˜ íƒ„ì†Œë°°ì¶œëŸ‰ ê´€ë¦¬ì™€ ê°ì¶• ëª©í‘œëŠ” ì–´ë–»ê²Œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì–´ë–¤ ê°ì¶• ì „ëµì„ ì‚¬ìš©í•˜ê³  ìˆë‚˜ìš”?"
    
    ì…ë ¥: "ì§€ë°°êµ¬ì¡°"
    ì¶œë ¥: "ê¸°ì—…ì˜ ì´ì‚¬íšŒ êµ¬ì„±ê³¼ ìš´ì˜ì²´ê³„ëŠ” ì–´ë–»ê²Œ ë˜ì–´ìˆìœ¼ë©°, ì§€ë°°êµ¬ì¡°ì˜ íˆ¬ëª…ì„±ì„ ì–´ë–»ê²Œ í™•ë³´í•˜ê³  ìˆë‚˜ìš”?"
    """

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì„ í™•ì¥í•´ì£¼ì„¸ìš”: {query}"}
            ],
            temperature=0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            max_tokens=200
        )
        expanded_query = response.choices[0].message.content.strip()
        print(f"\nì›ë˜ ì§ˆë¬¸: {query}")
        print(f"í™•ì¥ëœ ì§ˆë¬¸: {expanded_query}\n")
        return expanded_query
    except Exception as e:
        print(f"ì§ˆë¬¸ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return query

def extract_metadata_filters(query: str) -> Dict[str, str]:
    """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ
       ë²¡í„° DBë¥¼ ê²€ìƒ‰í•  ë•Œ, ë” ì¢‹ì€ ì •í™•ë„ë¥¼ ìœ„í•´ í•„í„°ë§ í•„ìš”
       sections: í•„í„°ë˜ëŠ” ë‹¨ì–´ëŠ” ì¶”í›„ì— ì¶”ê°€ ê°€ëŠ¥
       
       ì˜ˆì‹œ:
        query: "íƒ„ì†Œë°°ì¶œëŸ‰ ê´€ë¦¬ ë°©ë²•"
        return: {"section": "Environment"}
        
        query: "cjì˜ í™˜ê²½ ê´€ë¦¬"
        return: {"section": "Environment", "source": "CJ"}
    """
    filters = {}
    
    # ì„¹ì…˜ í•„í„° ì¶”ì¶œ
    sections = {
        "Environment": ["í™˜ê²½", "environment", "ê¸°í›„", "íƒ„ì†Œ"],
        "Social": ["ì‚¬íšŒ", "social", "ì§ì›", "ì¸ê¶Œ", "ì•ˆì „"],
        "Governance": ["ì§€ë°°êµ¬ì¡°", "governance", "ìœ¤ë¦¬", "ì¤€ë²•"]
    }
    
    query_lower = query.lower()
    for section, keywords in sections.items():
        if any(keyword in query_lower for keyword in keywords):
            filters["section"] = section
            break
    
    # íšŒì‚¬ëª…ìœ¼ë¡œ í•„í„°ë§
    companies = {
        "KTNG": ["ktng", "ì¼€ì´í‹°ì•¤ì§€", "kt&g"],
        "CJ": ["cj", "ì”¨ì œì´"],
        "SHINHAN": ["ì‹ í•œ", "shinhan"],
        "SAMPYO": ["ì‚¼í‘œ", "sampyo"]
    }
    
    for company, keywords in companies.items():
        if any(keyword in query_lower for keyword in keywords):
            filters["source"] = company
            break
    
    return filters

def get_relevance_label(score: float) -> str:
    """ê´€ë ¨ë„ ì ìˆ˜ì— ë”°ë¥¸ ë ˆì´ë¸” ë°˜í™˜"""
    if score > 0.9:
        return "ë§¤ìš° ë†’ì€ ê´€ë ¨ì„± ğŸŒŸ"
    elif score > 0.7:
        return "ë†’ì€ ê´€ë ¨ì„± â­"
    elif score > 0.5:
        return "ì¤‘ê°„ ê´€ë ¨ì„± âœ¨"
    else:
        return "ë‚®ì€ ê´€ë ¨ì„± âšª"

def rerank_documents(query: str, documents: List[str], metadata_list: List[Dict], top_k: int = 5) -> tuple:
    """Cross-encoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ì •ê·œí™” í¬í•¨)"""
    # ê° ë¬¸ì„œì™€ ì¿¼ë¦¬ì˜ ìŒì„ ìƒì„±
    pairs = [[query, doc] for doc in documents]
    
    # Cross-encoderë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (Tensorë¡œ ë°˜í™˜)
    raw_scores = cross_encoder.predict(pairs, convert_to_numpy=False)
    
    # sigmoidë¡œ ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
    if not isinstance(raw_scores, torch.Tensor):
        raw_scores = torch.tensor(raw_scores)
    norm_scores = sigmoid(raw_scores).tolist()
    
    # ì ìˆ˜ì— ë”°ë¼ ë¬¸ì„œ ì •ë ¬
    doc_score_pairs = list(zip(documents, metadata_list, norm_scores))
    doc_score_pairs.sort(key=lambda x: x[2], reverse=True)
    
    # top_k ê°œì˜ ë¬¸ì„œ ì„ íƒ
    reranked_docs = []
    reranked_metadata = []
    reranked_scores = []
    
    for doc, metadata, score in doc_score_pairs[:top_k]:
        reranked_docs.append(doc)
        reranked_metadata.append(metadata)
        reranked_scores.append(score)
    
    return reranked_docs, reranked_metadata, reranked_scores

def get_relevant_context(query: str, collection, initial_k: int = 20, final_k: int = 5, metadata_filters: Optional[Dict[str, str]] = None) -> tuple:
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (2ë‹¨ê³„ ê²€ìƒ‰)"""
    try:
        # metadata_filtersë¥¼ ChromaDB where ì ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        where_clause = None
        if metadata_filters and len(metadata_filters) > 0:
            # ê° í•„ë“œì— ëŒ€í•´ $eq ì—°ì‚°ìë¥¼ ì‚¬ìš©í•œ ì¡°ê±´ ìƒì„±
            conditions = []
            for field, value in metadata_filters.items():
                conditions.append({
                    field: {"$eq": value}
                })
            
            # ì¡°ê±´ì´ í•˜ë‚˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—¬ëŸ¬ ê°œë©´ $andë¡œ ê²°í•©
            if len(conditions) == 1:
                where_clause = conditions[0]
            else:
                where_clause = {
                    "$and": conditions
                }
            
            print(f"ì ìš©ëœ ê²€ìƒ‰ í•„í„°: {where_clause}")  # ë””ë²„ê¹…ìš© ì¶œë ¥
        
        # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ initial_kê°œ ë¬¸ì„œ ê²€ìƒ‰
        results = collection.query(
            query_texts=[query],
            n_results=initial_k,
            where=where_clause
        )
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í•„í„° ì—†ì´ ë‹¤ì‹œ ê²€ìƒ‰
        if not results['documents'][0]:
            print("ì§€ì •ëœ í•„í„°ë¡œ ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            results = collection.query(
                query_texts=[query],
                n_results=initial_k
            )
    except Exception as e:
        print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        # ì˜¤ë¥˜ ë°œìƒì‹œ í•„í„° ì—†ì´ ê²€ìƒ‰
        results = collection.query(
            query_texts=[query],
            n_results=initial_k
        )
    
    # 2ë‹¨ê³„: Cross-encoderë¡œ ì¬ìˆœìœ„í™”
    if results['documents'][0]:
        reranked_docs, reranked_metadata, scores = rerank_documents(
            query,
            results['documents'][0],
            results['metadatas'][0],
            final_k
        )
    else:
        return "", {}
    
    # ë©”íƒ€ë°ì´í„° ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
    metadata_summary = {
        "sections": set(),
        "subsections": set(),
        "sources": set(),
        "page_ranges": set()
    }
    
    contexts = []
    for i, (doc, metadata, score) in enumerate(zip(reranked_docs, reranked_metadata, scores)):
        # ë©”íƒ€ë°ì´í„° ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
        metadata_summary["sections"].add(metadata['section'])
        metadata_summary["subsections"].add(metadata['sub_section'])
        metadata_summary["sources"].add(metadata['source'])
        metadata_summary["page_ranges"].add(metadata.get('page_range', 'ì•Œ ìˆ˜ ì—†ìŒ'))
        
        # ê´€ë ¨ë„ ë ˆì´ë¸” ìƒì„±
        relevance_label = get_relevance_label(score)
        
        # ë¬¸ë§¥ êµ¬ì„± (ìœ ì‚¬ë„ ì ìˆ˜ì™€ ë ˆì´ë¸” í¬í•¨)
        context = f"""
                    ì¶œì²˜: {metadata['source']}
                    ì„¹ì…˜: {metadata['section']}
                    ì„œë¸Œì„¹ì…˜: {metadata['sub_section']}
                    í˜ì´ì§€: {metadata.get('page_range', 'ì•Œ ìˆ˜ ì—†ìŒ')}
                    ê´€ë ¨ë„: {score:.4f} ({relevance_label})
                    ë‚´ìš©: {doc}
                    ---"""
        contexts.append(context)
    
    return "\n".join(contexts), metadata_summary

def generate_response(query: str, context: str, metadata_summary: Dict):

    # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ few-shot ì˜ˆì‹œ ë¡œë“œ
    examples_path = Path(__file__).resolve().parent.parent / "data/few_shot_examples.json"
    with open(examples_path, "r", encoding="utf-8") as f:
        few_shot_examples = json.load(f)

    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    # ë©”íƒ€ë°ì´í„° ìš”ì•½ ë¬¸ìì—´ ìƒì„±
    metadata_info = f"""
    ì°¸ê³ í•œ ë¬¸ì„œ ì •ë³´:
    - ì„¹ì…˜: {', '.join(metadata_summary['sections'])}
    - ì„œë¸Œì„¹ì…˜: {', '.join(metadata_summary['subsections'])}
    - ì¶œì²˜: {', '.join(metadata_summary['sources'])}
    - í˜ì´ì§€: {', '.join(metadata_summary['page_ranges'])}
    """
    
    system_prompt = f"""
    ë‹¹ì‹ ì€ ê¸°ì—…ì˜ ESG(í™˜ê²½Â·ì‚¬íšŒÂ·ì§€ë°°êµ¬ì¡°) ê²½ì˜ ë„ì…ì„ ì§€ì›í•˜ëŠ” RAG(Retrieval-Augmented Generation) ê¸°ë°˜ì˜ AI ì±—ë´‡ì…ë‹ˆë‹¤.  
    í•­ìƒ ì œê³µëœ ë¬¸ì„œ(Context) ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ íƒ€ë‹¹ì„±ì„ ë³´ì™„í•˜ì„¸ìš”, ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

    ---

    ### ì‘ë‹µ ë°©ì‹

    1. ì‚¬ìš©ì ì§ˆë¬¸ì˜ **ìœ í˜•ì„ ë‚´ë¶€ì ìœ¼ë¡œ ë¶„ì„**í•©ë‹ˆë‹¤:
        - ì •ì˜í˜•: ê°œë…, ìš©ì–´ ì„¤ëª…
        - ì‹¤í–‰í˜•: ì „ëµ, KPI, ì‹¤í–‰ ë¡œë“œë§µ ìš”ì²­
        - ì‚¬ë¡€í˜•: ì‹¤ì œ ê¸°ì—… ì‚¬ë¡€ ìš”ì²­
        - ê¸°íƒ€: ì¼ë°˜ ì§ˆë¬¸ ë˜ëŠ” ë§¥ë½ ìš”ì•½

    2. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ **í•„ìš”í•œ ì„¹ì…˜ë§Œ ì„ íƒí•˜ì—¬ ì‘ë‹µ**í•˜ì„¸ìš”:
        - ì •ì˜í˜• â†’ ê°œìš”Â·ì •ì˜ + ë°°ê²½Â·ë§¥ë½
        - ì‹¤í–‰í˜• â†’ ê°œìš”Â·ì •ì˜ + ESG ì§€í‘œ + ì‹¤í–‰ ë¡œë“œë§µ
        - ì‚¬ë¡€í˜• â†’ ê°œìš”Â·ì •ì˜ + ë°°ê²½Â·ë§¥ë½ + ì‚¬ë¡€
        - ê¸°íƒ€ â†’ ê°œìš”Â·ì •ì˜ + ë°°ê²½Â·ë§¥ë½ (í•„ìš” ì‹œ ì¶”ê°€ ì„¹ì…˜ í¬í•¨ ê°€ëŠ¥)

    3. ëª¨ë“  ì‘ë‹µì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ **Markdown í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:

    ## 1. ê°œìš”Â·ì •ì˜  
    ## 2. ë°°ê²½Â·ë§¥ë½  
    ## 3. ESG í•µì‹¬ ì§€í‘œ (KPI)  
    ## 4. ë‹¨ê³„ë³„ ì‹¤í–‰ ë¡œë“œë§µ  
    ## 5. êµ¬ì²´ì  ì‚¬ë¡€  
    ## 6. ë¬¸ì„œ ì¶œì²˜ ìš”ì•½  
    **ğŸ“Œ ìš”ì•½: ...**

    4. ì¶œì²˜ëŠ” í•­ìƒ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”.  
    ì˜ˆ: (ì¶œì²˜: SHINHAN ESG ë³´ê³ ì„œ, p.4~5)


    ê´€ë ¨ ë¬¸ì„œ ì •ë³´:
    {metadata_info}

    ê´€ë ¨ ë¬¸ì„œ:
    {context}"""

    try:
        result = client.chat.completions.create(
            model="ft:gpt-3.5-turbo-0125:personal::BdZzCnDt",  # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ID
            messages = [{"role": "system", "content": system_prompt}] + 
                        few_shot_examples + 
                        [{"role": "user", "content": query}],
            temperature=0.7,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            max_tokens=1000    # ë” ê¸´ ì‘ë‹µ í—ˆìš©
        )
        return result.choices[0].message.content, metadata_info
    except Exception as e:
        print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", metadata_info

def main():
    print("ESG ì±—ë´‡ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
    
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(path="./data/chromadb")
    
    # ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • -> ChromaDBì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© í•¨ìˆ˜
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )
    
    # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
    collection = chroma_client.get_collection(
        name="ppt_documents_collection",
        embedding_function=embedding_function
    )
    
    print("ì´ˆê¸°í™” ì™„ë£Œ! ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
    # print("íŠ¹ì • ì˜ì—­(Environmental/Social/Governance)ì— ëŒ€í•´ ë¬¼ì–´ë³´ì‹œë©´ í•´ë‹¹ ì˜ì—­ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥): ")
        
        if query.lower() == 'quit':
            break
            
        # ì§§ì€ ì§ˆë¬¸ í™•ì¥
        expanded_query = expand_query(query)
        
        # ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ (ì›ë˜ ì§ˆë¬¸ì—ì„œ)
        metadata_filters = extract_metadata_filters(query)
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í™•ì¥ëœ ì§ˆë¬¸ ì‚¬ìš©)
        context, metadata_summary = get_relevant_context(
            expanded_query, 
            collection,
            metadata_filters=metadata_filters
        )
        
        # ì‘ë‹µ ìƒì„± (ì›ë˜ ì§ˆë¬¸ ì‚¬ìš©)
        response, metadata_info = generate_response(query, context, metadata_summary)
        
        print("\në‹µë³€:")
        print(response)
        
        # ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œë ¥ ì—¬ë¶€ í™•ì¸
        show_sources = input("\nì°¸ê³ í•œ ë¬¸ì„œ ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if show_sources.lower() == 'y':
            print("\nì°¸ê³ í•œ ë¬¸ì„œ ìš”ì•½:")
            print(metadata_info)
            print("\nìƒì„¸ ë¬¸ì„œ:")
            print(context)

if __name__ == "__main__":
    main() 