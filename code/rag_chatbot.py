from openai import OpenAI
from dotenv import load_dotenv
import os
import chromadb
from chromadb.utils import embedding_functions
import json
from typing import Optional, Dict, List
from sentence_transformers import CrossEncoder
import numpy as np
import torch
from torch.nn.functional import sigmoid

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

client = OpenAI(api_key=api_key)

# Cross-encoder ëª¨ë¸ ì´ˆê¸°í™”
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def expand_query(query: str, min_length: int = 10) -> str:
    """ì§§ì€ ì¿¼ë¦¬ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ í™•ì¥"""
    if len(query) > min_length:
        return query
        
    system_prompt = """ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§§ì€ ì§ˆë¬¸ì„ ESG ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë” êµ¬ì²´ì ì´ê³  í’ë¶€í•˜ê²Œ ë°”ê¿”ì£¼ëŠ” ì „ë¬¸ê°€ì•¼.
    ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì•¼ í•´:
    1. ì›ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í™•ì¥
    2. ESG ê´€ë ¨ êµ¬ì²´ì ì¸ ìš©ì–´ë‚˜ ê°œë… í¬í•¨
    3. ê²€ìƒ‰ì— ë„ì›€ë  ë§Œí•œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
    4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
    5. í™•ì¥ëœ ì§ˆë¬¸ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œí•œ
    
    ì˜ˆì‹œ:
    ì…ë ¥: "íƒ„ì†Œë°°ì¶œëŸ‰?"
    ì¶œë ¥: "ê¸°ì—…ì˜ íƒ„ì†Œë°°ì¶œëŸ‰ ê´€ë¦¬ì™€ ê°ì¶• ëª©í‘œëŠ” ì–´ë–»ê²Œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì–´ë–¤ ê°ì¶• ì „ëµì„ ì‚¬ìš©í•˜ê³  ìˆë‚˜ìš”?"
    
    ì…ë ¥: "ì§€ë°°êµ¬ì¡°"
    ì¶œë ¥: "ê¸°ì—…ì˜ ì´ì‚¬íšŒ êµ¬ì„±ê³¼ ìš´ì˜ì²´ê³„ëŠ” ì–´ë–»ê²Œ ë˜ì–´ìˆìœ¼ë©°, ì§€ë°°êµ¬ì¡°ì˜ íˆ¬ëª…ì„±ì„ ì–´ë–»ê²Œ í™•ë³´í•˜ê³  ìˆë‚˜ìš”?"
    """

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì„ í™•ì¥í•´ì£¼ì„¸ìš”: {query}"}
            ],
            temperature=0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            max_tokens=200
        )
        expanded_query = response.choices[0].message.content.strip()
        print(f"\nì›ë˜ ì§ˆë¬¸: {query}")
        print(f"í™•ì¥ëœ ì§ˆë¬¸: {expanded_query}\n")
        return expanded_query
    except Exception as e:
        print(f"ì§ˆë¬¸ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return query

def extract_metadata_filters(query: str) -> Dict[str, str]:
    """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ"""
    filters = {}
    
    # ì„¹ì…˜ í•„í„° ì¶”ì¶œ
    sections = {
        "Environment": ["í™˜ê²½", "environment", "ê¸°í›„", "íƒ„ì†Œ"],
        "Social": ["ì‚¬íšŒ", "social", "ì§ì›", "ì¸ê¶Œ", "ì•ˆì „"],
        "Governance": ["ì§€ë°°êµ¬ì¡°", "governance", "ìœ¤ë¦¬", "ì¤€ë²•"]
    }
    
    query_lower = query.lower()
    for section, keywords in sections.items():
        if any(keyword in query_lower for keyword in keywords):
            filters["section"] = section
            break
    
    # íšŒì‚¬ëª…ìœ¼ë¡œ í•„í„°ë§
    companies = {
        "KTNG": ["ktng", "ì¼€ì´í‹°ì•¤ì§€", "kt&g"],
        "CJ": ["cj", "ì”¨ì œì´"],
        "SHINHAN": ["ì‹ í•œ", "shinhan"],
        "SAMPYO": ["ì‚¼í‘œ", "sampyo"]
    }
    
    for company, keywords in companies.items():
        if any(keyword in query_lower for keyword in keywords):
            filters["source"] = company
            break
    
    return filters

def get_relevance_label(score: float) -> str:
    """ê´€ë ¨ë„ ì ìˆ˜ì— ë”°ë¥¸ ë ˆì´ë¸” ë°˜í™˜"""
    if score > 0.9:
        return "ë§¤ìš° ë†’ì€ ê´€ë ¨ì„± ğŸŒŸ"
    elif score > 0.7:
        return "ë†’ì€ ê´€ë ¨ì„± â­"
    elif score > 0.5:
        return "ì¤‘ê°„ ê´€ë ¨ì„± âœ¨"
    else:
        return "ë‚®ì€ ê´€ë ¨ì„± âšª"

def rerank_documents(query: str, documents: List[str], metadata_list: List[Dict], top_k: int = 5) -> tuple:
    """Cross-encoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ì •ê·œí™” í¬í•¨)"""
    # ê° ë¬¸ì„œì™€ ì¿¼ë¦¬ì˜ ìŒì„ ìƒì„±
    pairs = [[query, doc] for doc in documents]
    
    # Cross-encoderë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (Tensorë¡œ ë°˜í™˜)
    raw_scores = cross_encoder.predict(pairs, convert_to_numpy=False)
    
    # sigmoidë¡œ ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
    if not isinstance(raw_scores, torch.Tensor):
        raw_scores = torch.tensor(raw_scores)
    norm_scores = sigmoid(raw_scores).tolist()
    
    # ì ìˆ˜ì— ë”°ë¼ ë¬¸ì„œ ì •ë ¬
    doc_score_pairs = list(zip(documents, metadata_list, norm_scores))
    doc_score_pairs.sort(key=lambda x: x[2], reverse=True)
    
    # top_k ê°œì˜ ë¬¸ì„œ ì„ íƒ
    reranked_docs = []
    reranked_metadata = []
    reranked_scores = []
    
    for doc, metadata, score in doc_score_pairs[:top_k]:
        reranked_docs.append(doc)
        reranked_metadata.append(metadata)
        reranked_scores.append(score)
    
    return reranked_docs, reranked_metadata, reranked_scores

def get_relevant_context(query: str, collection, initial_k: int = 20, final_k: int = 5, metadata_filters: Optional[Dict[str, str]] = None) -> tuple:
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (2ë‹¨ê³„ ê²€ìƒ‰)"""
    try:
        # metadata_filtersë¥¼ ChromaDB where ì ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        where_clause = None
        if metadata_filters and len(metadata_filters) > 0:
            # ê° í•„ë“œì— ëŒ€í•´ $eq ì—°ì‚°ìë¥¼ ì‚¬ìš©í•œ ì¡°ê±´ ìƒì„±
            conditions = []
            for field, value in metadata_filters.items():
                conditions.append({
                    field: {"$eq": value}
                })
            
            # ì¡°ê±´ì´ í•˜ë‚˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—¬ëŸ¬ ê°œë©´ $andë¡œ ê²°í•©
            if len(conditions) == 1:
                where_clause = conditions[0]
            else:
                where_clause = {
                    "$and": conditions
                }
            
            print(f"ì ìš©ëœ ê²€ìƒ‰ í•„í„°: {where_clause}")  # ë””ë²„ê¹…ìš© ì¶œë ¥
        
        # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ initial_kê°œ ë¬¸ì„œ ê²€ìƒ‰
        results = collection.query(
            query_texts=[query],
            n_results=initial_k,
            where=where_clause
        )
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í•„í„° ì—†ì´ ë‹¤ì‹œ ê²€ìƒ‰
        if not results['documents'][0]:
            print("ì§€ì •ëœ í•„í„°ë¡œ ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            results = collection.query(
                query_texts=[query],
                n_results=initial_k
            )
    except Exception as e:
        print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        # ì˜¤ë¥˜ ë°œìƒì‹œ í•„í„° ì—†ì´ ê²€ìƒ‰
        results = collection.query(
            query_texts=[query],
            n_results=initial_k
        )
    
    # 2ë‹¨ê³„: Cross-encoderë¡œ ì¬ìˆœìœ„í™”
    if results['documents'][0]:
        reranked_docs, reranked_metadata, scores = rerank_documents(
            query,
            results['documents'][0],
            results['metadatas'][0],
            final_k
        )
    else:
        return "", {}
    
    # ë©”íƒ€ë°ì´í„° ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
    metadata_summary = {
        "sections": set(),
        "subsections": set(),
        "sources": set(),
        "page_ranges": set()
    }
    
    contexts = []
    for i, (doc, metadata, score) in enumerate(zip(reranked_docs, reranked_metadata, scores)):
        # ë©”íƒ€ë°ì´í„° ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
        metadata_summary["sections"].add(metadata['section'])
        metadata_summary["subsections"].add(metadata['sub_section'])
        metadata_summary["sources"].add(metadata['source'])
        metadata_summary["page_ranges"].add(metadata.get('page_range', 'ì•Œ ìˆ˜ ì—†ìŒ'))
        
        # ê´€ë ¨ë„ ë ˆì´ë¸” ìƒì„±
        relevance_label = get_relevance_label(score)
        
        # ë¬¸ë§¥ êµ¬ì„± (ìœ ì‚¬ë„ ì ìˆ˜ì™€ ë ˆì´ë¸” í¬í•¨)
        context = f"""
                    ì¶œì²˜: {metadata['source']}
                    ì„¹ì…˜: {metadata['section']}
                    ì„œë¸Œì„¹ì…˜: {metadata['sub_section']}
                    í˜ì´ì§€: {metadata.get('page_range', 'ì•Œ ìˆ˜ ì—†ìŒ')}
                    ê´€ë ¨ë„: {score:.4f} ({relevance_label})
                    ë‚´ìš©: {doc}
                    ---"""
        contexts.append(context)
    
    return "\n".join(contexts), metadata_summary

def generate_response(query: str, context: str, metadata_summary: Dict):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    # ë©”íƒ€ë°ì´í„° ìš”ì•½ ë¬¸ìì—´ ìƒì„±
    metadata_info = f"""
    ì°¸ê³ í•œ ë¬¸ì„œ ì •ë³´:
    - ì„¹ì…˜: {', '.join(metadata_summary['sections'])}
    - ì„œë¸Œì„¹ì…˜: {', '.join(metadata_summary['subsections'])}
    - ì¶œì²˜: {', '.join(metadata_summary['sources'])}
    - í˜ì´ì§€: {', '.join(metadata_summary['page_ranges'])}
    """
    
    system_prompt = f"""ë„ˆëŠ” ESG ì „ë¬¸ê°€ì•¼. 
ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜. 
ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•´:

1. ë‹µë³€ì€ í•­ìƒ êµ¬ì²´ì ì´ê³  ìƒì„¸í•´ì•¼ í•¨
2. ê°€ëŠ¥í•œ í•œ ë§ì€ ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•¨
3. ì¶œì²˜ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë©´ì„œ ë‹µë³€í•´ì•¼ í•¨
4. ìˆ«ì, í†µê³„, êµ¬ì²´ì ì¸ ì •ì±…ì´ë‚˜ í”„ë¡œê·¸ë¨ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨
5. ë‹µë³€ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•¨:
   - í•µì‹¬ ë‹µë³€ ìš”ì•½
   - êµ¬ì²´ì ì¸ ë‚´ìš© ì„¤ëª…
   - ê´€ë ¨ ì •ì±…ì´ë‚˜ í”„ë¡œê·¸ë¨
   - ì„±ê³¼ë‚˜ ëª©í‘œ (ìˆëŠ” ê²½ìš°)

ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë²—ì–´ë‚˜ëŠ” ë‹µë³€ì€ í•˜ì§€ ë§ê³ , ëª¨ë¥´ëŠ” ê²½ìš° ëª¨ë¥¸ë‹¤ê³  í•´ì•¼ í•´.

{metadata_info}

ê´€ë ¨ ë¬¸ì„œ:
{context}"""

    try:
        result = client.chat.completions.create(
            model="ft:gpt-3.5-turbo-0125:personal::BcpKR8kJ",  # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ID
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature=0.3,  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
            max_tokens=300    # ë” ê¸´ ì‘ë‹µ í—ˆìš©
        )
        return result.choices[0].message.content, metadata_info
    except Exception as e:
        print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", metadata_info

def main():
    print("ESG ì±—ë´‡ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")
    
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(path="./outputs/chromadb")
    
    # ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )
    
    # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
    collection = chroma_client.get_collection(
        name="ppt_documents_collection",
        embedding_function=embedding_function
    )
    
    print("ì´ˆê¸°í™” ì™„ë£Œ! ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
    # print("íŠ¹ì • ì˜ì—­(Environmental/Social/Governance)ì— ëŒ€í•´ ë¬¼ì–´ë³´ì‹œë©´ í•´ë‹¹ ì˜ì—­ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    
    while True:
        # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
        query = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥): ")
        
        if query.lower() == 'quit':
            break
            
        # ì§§ì€ ì§ˆë¬¸ í™•ì¥
        expanded_query = expand_query(query)
        
        # ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ (ì›ë˜ ì§ˆë¬¸ì—ì„œ)
        metadata_filters = extract_metadata_filters(query)
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í™•ì¥ëœ ì§ˆë¬¸ ì‚¬ìš©)
        context, metadata_summary = get_relevant_context(
            expanded_query, 
            collection,
            metadata_filters=metadata_filters
        )
        
        # ì‘ë‹µ ìƒì„± (ì›ë˜ ì§ˆë¬¸ ì‚¬ìš©)
        response, metadata_info = generate_response(query, context, metadata_summary)
        
        print("\në‹µë³€:")
        print(response)
        
        # ì‚¬ìš©ëœ ë¬¸ì„œ ì¶œë ¥ ì—¬ë¶€ í™•ì¸
        show_sources = input("\nì°¸ê³ í•œ ë¬¸ì„œ ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
        if show_sources.lower() == 'y':
            print("\nì°¸ê³ í•œ ë¬¸ì„œ ìš”ì•½:")
            print(metadata_info)
            print("\nìƒì„¸ ë¬¸ì„œ:")
            print(context)

if __name__ == "__main__":
    main() 